{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be7ba8d-401d-4019-9ece-ff66c47072cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1d919e-20d9-4a40-814d-bdf61372b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle to dataframe\n",
    "filename = 'hansard-speeches-post2010.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2c6577-437b-4928-b1e6-523015f83167",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "318b0d66-4b5e-4a26-bf94-7c43ddae6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc, stop_words=nlp.Defaults.stop_words):\n",
    "    '''Takes a spacy doc and lemmatizes each of the tokens\n",
    "    \n",
    "    Takes alphanumeric tokens whose lowercase form does not appear in stop_words\n",
    "    and returns their lowercase lemmatized form\n",
    "    '''\n",
    "    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and token.text.lower() not in stop_words]\n",
    "    return lemmas\n",
    "\n",
    "def process_chunk(texts, batch_size=100):\n",
    "    '''returns list, each item of which is a list of lemmas'''\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        preproc_pipe.append(lemmatize(doc))\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46683f-4931-4f8f-898c-3d0dafa39c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below follows code for parallelization: not supported on GPU\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    '''Takes an iterable and returns a list of chunks of the iterable'''\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    '''Flatten a list of lists to a list'''\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_parallel(texts, total_length, chunksize=100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer='processes')\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, total_length=total_length, chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05232a12-7fee-4607-bfd1-be7f44489cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.1 s ± 556 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a = process_chunk(df.sample(10000).speech, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5758b17b-a212-42b0-8576-0be981939f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.945509333333334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((len(df)/10000)*45.1)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efa73ce6-7930-4026-af55-3ff070460693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['lemmas'] = process_chunk(df.speech, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a174365-f75a-4115-88f3-a5eb657a5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hansard-speeches-post2010-lemmatized.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
