{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f0296c-0592-49bd-b9ed-15f183f22aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262e776-49f2-4243-9fff-65826c968e6f",
   "metadata": {},
   "source": [
    "### load lemmatized speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13b7c85e-c7f1-4f06-b165-2fead2311a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hansard-speeches-post2010-lemmatized.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c48eb8-4f11-48b6-b0aa-86af74156015",
   "metadata": {},
   "source": [
    "### replace empty entries with ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63280237-f392-487e-9701-f5378d137a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['major_heading', 'minor_heading']] = df[['major_heading', 'minor_heading']].fillna(value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3bf66-e660-4390-84eb-cfe559809fad",
   "metadata": {},
   "source": [
    "### combine major and minor headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b9d1c55-cdae-48f6-b609-90b93afdd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['minor_heading'] = df['minor_heading'].str.strip()\n",
    "df['major_heading'] = df['major_heading'].str.strip()\n",
    "df['heading'] = df['major_heading'] + ' ' + df['minor_heading']\n",
    "df['heading'] = df['heading'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19468c00-1601-4851-b0c6-d700af7a6144",
   "metadata": {},
   "source": [
    "### aggregate entries\n",
    "\n",
    "rows with identical date, heading, and display_as index values are combined together: speeches by string joining with spaces, lemmas by list concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ddc3a36-415a-4da1-961a-a44bfb3cce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.groupby(['date', 'heading', 'display_as']).agg({'speech': lambda x: \" \".join(x), 'lemmas': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a24c0113-7b09-45fa-b216-03ef3662b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hansard-speeches-post2010-lemmatized-agg.pkl', 'wb') as f:\n",
    "    pickle.dump(df_agg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e850fbf-6d22-487e-ab0c-7c43ed1d44bd",
   "metadata": {},
   "source": [
    "### make n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dfa645c-da37-4c2c-8d30-96d5e2d95895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 2-gram model\n",
      "done 3-gram model\n",
      "done 4-gram model\n",
      "done 5-gram model\n",
      "done 6-gram model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "# up to 6 long\n",
    "max_n_gram = 6\n",
    "\n",
    "ngram_models = {}\n",
    "\n",
    "texts = df_agg.lemmas.values\n",
    "\n",
    "for n in np.arange(2, max_n_gram+1):\n",
    "    ngram_models[n] = Phrases(texts,\n",
    "                              min_count=300, # individual words and n_gram used at least 300 times\n",
    "                              threshold=30, # score produced by Phrases - manual testing, 25 a conservative balance\n",
    "                              connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "    texts = [ngram_models[n][text] for text in texts]\n",
    "    print(f'done {n}-gram model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf741ac4-ae49-40b4-82dd-046a3a58f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg['lemmas_ngrams'] = texts\n",
    "df_agg = df_agg.drop(columns='lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a7d427-a23c-4b59-a50d-bd2cc0aabe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases = ngram_models[6].export_phrases()\n",
    "# sorted_phrases_asc = dict(sorted(phrases.items(), key=lambda item: item[1]))\n",
    "# sorted_phrases_desc = dict(sorted(phrases.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40cfbb6-af36-49d2-b187-8a302ef137e7",
   "metadata": {},
   "source": [
    "## remove lemmas present in >X% speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47a92732-e36b-4d5f-9642-52ead58745a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit cv\n",
      "found stopwords and removed from texts (lists of lemmas)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dummy = lambda x: x\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    max_df = 0.20\n",
    "    )\n",
    "\n",
    "texts = df_agg.lemmas_ngrams.values\n",
    "cv.fit(texts)\n",
    "print(\"fit cv\")\n",
    "\n",
    "stopwords = list(cv.stop_words_) + ['', ' ']\n",
    "texts = [[w for w in text if w not in stopwords] for text in texts]\n",
    "print('found stopwords and removed from texts (lists of lemmas)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93065247-69cc-4d71-8e0d-49a73752153c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right',\n",
       " 'say',\n",
       " 'point',\n",
       " 'come',\n",
       " 'country',\n",
       " 'friend',\n",
       " 'hon',\n",
       " 'people',\n",
       " 'give',\n",
       " 'support',\n",
       " 'agree',\n",
       " 'government',\n",
       " 'time',\n",
       " 'need',\n",
       " 'take',\n",
       " 'way',\n",
       " 'issue',\n",
       " 'secretary_state',\n",
       " 'good',\n",
       " 'work',\n",
       " 'member',\n",
       " 'year',\n",
       " 'minister',\n",
       " 'know',\n",
       " 'want',\n",
       " 'house',\n",
       " '',\n",
       " ' ']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5caaa8-17d9-45fe-be8f-02fd588e1a67",
   "metadata": {},
   "source": [
    "* In parliament convention is not to refer to other MPs by name, or using second person pronouns. This leads to stopwords like 'right', 'hon', 'member', 'friend' (e.g. \"does my right hon. friend agree that...\" vs \"Theresa May, do you agree that...\") and 'minister', (\"the minister for vaccines\"\n",
    "* Others related to typical non-topical political rhetoric, e.g. 'government', 'people', 'need', 'work'\n",
    "* Others related to parliamentary procedure, e.g. 'time', 'house', 'year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b48d069-b6f4-463b-9751-0538b02f712d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate dataframe for stopword removal - quicker testing due to time of n-grams creation\n",
    "df_agg_sw = df_agg.copy()\n",
    "df_agg_sw['lemmas_ngrams'] = texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684c207-b00a-48e0-8cdf-33cb61378371",
   "metadata": {},
   "source": [
    "### remove speeches with fewer than 40 lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2af1f16e-65c9-4b59-b28c-0a472536e72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3924887659315881"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no. entries in df\n",
    "total_l = len(df_agg_sw)\n",
    "\n",
    "# minimum no. lemmas for passed speeches\n",
    "t = 40\n",
    "\n",
    "# filter\n",
    "# again, separate dataframe for quicker testing at memory expense\n",
    "df_agg_t = df_agg_sw[df_agg_sw.lemmas_ngrams.apply(len) >= t]\n",
    "\n",
    "# proportion entries retained\n",
    "len(df_agg_t) / total_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82e42a15-4a1a-42c7-b097-a6061e9b4af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>lemmas_ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>heading</th>\n",
       "      <th>display_as</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-05-25</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">DEPUTY SPEAKERS Debate on the Address — [1st Day]</th>\n",
       "      <th>Alan Beith</th>\n",
       "      <td>I usually enjoy and can be quite entertained b...</td>\n",
       "      <td>[usually, enjoy, entertain, speech, manchester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andrew Miller</th>\n",
       "      <td>On a point of order, Mr. Speaker.It might help...</td>\n",
       "      <td>[order, speaker, help, particularly, seek, cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andrew Selous</th>\n",
       "      <td>I want to pick the right hon. Gentleman up on ...</td>\n",
       "      <td>[pick, gentleman, different, talk, health, spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anne Begg</th>\n",
       "      <td>I pay tribute to the new hon. Member for Watfo...</td>\n",
       "      <td>[pay_tribute, new, watford, richard, harringto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charles Walker</th>\n",
       "      <td>Mr Deputy Speaker, thank you for calling me to...</td>\n",
       "      <td>[mr_deputy_speaker, thank, call, speak, day, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-11-05</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Valedictory Debate</th>\n",
       "      <th>Seema Kennedy</th>\n",
       "      <td>On a point of order, Madam Deputy Speaker. As ...</td>\n",
       "      <td>[order, madam_deputy_speaker, order, seek, gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stephen Pound</th>\n",
       "      <td>May I begin by apologising, Madam Deputy Speak...</td>\n",
       "      <td>[begin, apologise, madam_deputy_speaker, north...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stephen Twigg</th>\n",
       "      <td>It is a pleasure to follow the hon. Member for...</td>\n",
       "      <td>[pleasure_follow, north, devon, peter, heaton,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Teresa Pearce</th>\n",
       "      <td>I would like to thank my fantastic family, my ...</td>\n",
       "      <td>[like, thank, fantastic, family, staff, amazin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valerie Vaz</th>\n",
       "      <td>Thank you, Mr Speaker. I congratulate you on y...</td>\n",
       "      <td>[thank, mr_speaker, congratulate, victory, spe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98174 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        speech  \\\n",
       "date       heading                                           display_as                                                          \n",
       "2010-05-25 DEPUTY SPEAKERS Debate on the Address — [1st Day] Alan Beith      I usually enjoy and can be quite entertained b...   \n",
       "                                                             Andrew Miller   On a point of order, Mr. Speaker.It might help...   \n",
       "                                                             Andrew Selous   I want to pick the right hon. Gentleman up on ...   \n",
       "                                                             Anne Begg       I pay tribute to the new hon. Member for Watfo...   \n",
       "                                                             Charles Walker  Mr Deputy Speaker, thank you for calling me to...   \n",
       "...                                                                                                                        ...   \n",
       "2019-11-05 Valedictory Debate                                Seema Kennedy   On a point of order, Madam Deputy Speaker. As ...   \n",
       "                                                             Stephen Pound   May I begin by apologising, Madam Deputy Speak...   \n",
       "                                                             Stephen Twigg   It is a pleasure to follow the hon. Member for...   \n",
       "                                                             Teresa Pearce   I would like to thank my fantastic family, my ...   \n",
       "                                                             Valerie Vaz     Thank you, Mr Speaker. I congratulate you on y...   \n",
       "\n",
       "                                                                                                                 lemmas_ngrams  \n",
       "date       heading                                           display_as                                                         \n",
       "2010-05-25 DEPUTY SPEAKERS Debate on the Address — [1st Day] Alan Beith      [usually, enjoy, entertain, speech, manchester...  \n",
       "                                                             Andrew Miller   [order, speaker, help, particularly, seek, cat...  \n",
       "                                                             Andrew Selous   [pick, gentleman, different, talk, health, spe...  \n",
       "                                                             Anne Begg       [pay_tribute, new, watford, richard, harringto...  \n",
       "                                                             Charles Walker  [mr_deputy_speaker, thank, call, speak, day, q...  \n",
       "...                                                                                                                        ...  \n",
       "2019-11-05 Valedictory Debate                                Seema Kennedy   [order, madam_deputy_speaker, order, seek, gui...  \n",
       "                                                             Stephen Pound   [begin, apologise, madam_deputy_speaker, north...  \n",
       "                                                             Stephen Twigg   [pleasure_follow, north, devon, peter, heaton,...  \n",
       "                                                             Teresa Pearce   [like, thank, fantastic, family, staff, amazin...  \n",
       "                                                             Valerie Vaz     [thank, mr_speaker, congratulate, victory, spe...  \n",
       "\n",
       "[98174 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9892ff5f-44d5-450c-811a-a65fb428d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hansard-speeches-post2010-lemmatized-agg-ngrams.pkl', 'wb') as f:\n",
    "    pickle.dump(df_agg_t, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
